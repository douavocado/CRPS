data:
  # Use 64-dimensional data for an 8x8 spatial grid
  dimension: 64
  ar_order: 1
  n_timesteps: 100
  n_series: 1000
  noise_scale: 0.05
  input_timesteps: 1
  output_timesteps: 2
  random_state: 123
  split_ratios:
    train: 0.8
    val: 0.1
    test: 0.1
  
  noise_args:
    type: "spatial_rbf"
    length_scale: 2.0

  spatial_args:
    grid_size: [8, 8]
    conv_filters:
      - [[0.05, -0.1, 0.02],
         [-0.03, 0.15, -0.05],
         [0.04, -0.02, 0.06]]

experiment:
  name: "progressive_training_with_deactivation_example"
  save_dir: "results"
  save_model: true
  save_results: true
  log_interval: 10
  evaluate_test: true
  seeds: [42]
  parallel_seeds: false

loss:
  training_loss:
    loss_instances:
      # Example 1: Always active baseline loss
      - name: crps_baseline
        loss_function: crps_loss_general
        coefficient: 1.0
        spatial_pooling:
          enabled: false
        loss_args: {}
        # No progressive_training = always active
      
      # Example 2: Warm-up loss - active only during early training
      - name: energy_warmup
        loss_function: energy_score_loss
        coefficient: 0.5
        spatial_pooling:
          enabled: true
          pool_type: mean
          kernel_size: 4
          spatial_shape: [8, 8]
        loss_args:
          norm_dim: true
        progressive_training:
          # Active from start
          activation_conditions: []
          # Deactivate after epoch 20
          deactivation_conditions:
            - type: epoch_ge
              epoch: 20
      
      # Example 3: Temporary loss - active only between epochs 10-30
      - name: energy_temporary
        loss_function: energy_score_loss
        coefficient: 0.3
        spatial_pooling:
          enabled: false
        loss_args:
          norm_dim: true
        progressive_training:
          # Using epoch_between for both activation and deactivation
          activation_conditions:
            - type: epoch_between
              start_epoch: 10
              end_epoch: 30
          deactivation_conditions:
            - type: epoch_ge
              epoch: 30
      
      # Example 4: Plateau-triggered loss that deactivates when improvement resumes
      - name: variogram_plateau_helper
        loss_function: variogram_score_loss
        coefficient: 0.4
        spatial_pooling:
          enabled: false
        loss_args:
          p: 1.0
        progressive_training:
          # Activate when validation plateaus
          activation_conditions:
            - type: validation_plateau
              validation_loss_instance: crps_baseline
              plateau_epochs: 5
          # Deactivate when validation improves again
          deactivation_conditions:
            - type: validation_improved
              validation_loss_instance: crps_baseline
              improvement_window: 2  # Within 2 epochs of improvement
      
      # Example 5: Cyclic loss - can activate and deactivate multiple times
      - name: crps_cyclic
        loss_function: crps_loss_general
        coefficient: 0.2
        spatial_pooling:
          enabled: true
          pool_type: max
          kernel_size: 2
          spatial_shape: [8, 8]
        loss_args: {}
        progressive_training:
          # Reactivate whenever baseline plateaus for 3 epochs
          activation_conditions:
            - type: validation_plateau
              validation_loss_instance: crps_baseline
              plateau_epochs: 3
          # Deactivate after being active for 5 epochs
          deactivation_conditions:
            - type: validation_improved
              validation_loss_instance: crps_baseline
              improvement_window: 1
      
      # Example 6: Sequential dependency - active after another loss deactivates
      - name: energy_post_warmup
        loss_function: energy_score_loss
        coefficient: 0.6
        spatial_pooling:
          enabled: false
        loss_args:
          norm_dim: false
        progressive_training:
          # Activate 2 epochs after warmup loss deactivates
          activation_conditions:
            - type: loss_deactivated_for
              target_loss_instance: energy_warmup
              epochs_since_deactivation: 2
          # Optional: deactivate if validation loss gets very low
          deactivation_conditions:
            - type: epoch_ge
              epoch: 50
      
      # Example 7: Complex multi-condition with deactivation
      - name: kernel_advanced
        loss_function: kernel_score_loss
        coefficient: 0.3
        spatial_pooling:
          enabled: false
        loss_args:
          kernel_type: gaussian
          sigma: 1.0
        progressive_training:
          # Complex activation conditions
          activation_conditions:
            # ALL must be true
            - type: epoch_ge
              epoch: 15
            - type: loss_activated_for
              target_loss_instance: energy_temporary
              epochs_since_activation: 3
          # Deactivate based on multiple conditions
          deactivation_conditions:
            # ALL must be true
            - type: epoch_ge
              epoch: 40
            - type: validation_plateau
              validation_loss_instance: default
              plateau_epochs: 10

  validation_loss: null  # Will default to training_loss
  
  testing_loss:
    loss_instances:
      - name: crps_test
        loss_function: crps_loss_general
        coefficient: 1.0
        spatial_pooling:
          enabled: false
        loss_args: {}
      - name: energy_test
        loss_function: energy_score_loss
        coefficient: 1.0
        spatial_pooling:
          enabled: false
        loss_args:
          norm_dim: true

model:
  type: "mlp_sampler"
  
  fgn_encoder:
    activation_function: "relu"
    dropout_rate: 0.1
    hidden_size: 64
    input_size: null
    latent_dim: 16
    n_layers: 3
    non_linear: true
    output_size: null
    zero_inputs: false
  
  mlp_sampler:
    activation_function: "relu"
    dropout_rate: 0.1
    hidden_size: [128, 128]
    input_size: null
    latent_dim: 32
    n_layers: 3
    non_linear: true
    output_size: null
    sample_layer_index: 1
    zero_inputs: false
  
  affine_normal:
    output_dim: null

training:
  n_epochs: 60  # Sufficient to see activation/deactivation cycles
  learning_rate: 0.0001
  batch_size: 32
  n_samples: 10
  patience: 20  # Extended patience for progressive training
  device: "auto"
  num_workers: 4
  shuffle_train: true
  normalise_data: true
  grad_clip_norm: 0.5
  verbose: true
  
  optimizer: "adam"
  optimizer_args:
    weight_decay: 1e-6
    betas: [0.9, 0.999]
    eps: 1e-8

tracking:
  enabled: true
  track_every: 5
  sample_indices: [10, 11, 12, 13, 14]
  n_samples: 1000
  kde_bandwidth: "auto"
  contour_levels: [0.65, 0.95, 0.99]
  max_checkpoints: 5
  generate_plots_after_training: true
  create_animation: false 